# Deep-Learning-Courses

嗯，就是这个！a^b
### 4.3 - Forward and Backward propagation

Now that your parameters are initialized, you can do the "forward" and "backward" propagation steps for learning the parameters.

**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.

**Hints**:

Forward Propagation:
- You get X
- You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$
ax^{2} + by^{2} + c = 0\begin{pmatrix}
 a_{11} & a_{12} & a_{13}\\ 
 a_{21} & a_{22} & a_{23}\\ 
 a_{31} & a_{32} & a_{33}
 \end{pmatrix}
